---
---

@inproceedings{nguyen2025improved,
  title={Improved Training Technique for Shortcut Models},
  author={Anh Nguyen* and Viet Nguyen* and Duc Vu and Trung Dao and Chi Tran and Toan Tran and Anh Tran},
  booktitle={The Thirty-nine Annual Conference on Neural Information Processing Systems, 2025},
  year={2025},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/2510.21250v1},
  abstract={
  Shortcut models represent a promising, non-adversarial paradigm for generative modeling, uniquely supporting one-step, few-step, and multi-step sampling from a single trained network. However, their widespread adoption has been stymied by critical performance bottlenecks. This paper tackles the five core issues that held shortcut models back: (1) the hidden flaw of compounding guidance, which we are the first to formalize, causing severe image artifacts; (2) inflexible fixed guidance that restricts inference-time control; (3) a pervasive frequency bias driven by a reliance on low-level distances in the direct domain, which biases reconstructions toward low frequencies; (4) divergent self-consistency arising from a conflict with EMA training; and (5) curvy flow trajectories that impede convergence. To address these challenges, we introduce iSM, a unified training framework that systematically resolves each limitation. Our framework is built on four key improvements: Intrinsic Guidance provides explicit, dynamic control over guidance strength, resolving both compounding guidance and inflexibility. A Multi-Level Wavelet Loss mitigates frequency bias to restore high-frequency details. Scaling Optimal Transport (sOT) reduces training variance and learns straighter, more stable generative paths. Finally, a Twin EMA strategy reconciles training stability with self-consistency. Extensive experiments on ImageNet 256 Ã— 256 demonstrate that our approach yields substantial FID improvements over baseline shortcut models across one-step, few-step, and multi-step generation, making shortcut models a viable and competitive class of generative models.
  },
  selected={true},
  slides={/assets/publications/neurips2025/slides.pdf},
  poster={/assets/publications/neurips2025/poster.pdf},
  preview={neurips2025.jpg}
}

@inproceedings{nguyen2025supercharged,
  title={Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts},
  author={Viet Nguyen* and Anh Nguyen* and Trung Dao and Khoi Nguyen and Cuong Pham and Toan Tran and Anh Tran},
  booktitle={International Conference on Computer Vision, 2025},
  year={2025},
  abbr={ICCV},
  abstract={
  The escalating demand for real-time image synthesis has driven significant advancements in one-step diffusion models, which inherently offer expedited generation speeds compared to traditional multi-step methods. However, this enhanced efficiency is frequently accompanied by a compromise in the controllability of image attributes. While negative prompting, typically implemented via classifier-free guidance (CFG), has proven effective for fine-grained control in multi-step models, its application to one-step generators remains largely unaddressed. Due to the lack of iterative refinement, as in multi-step diffusion, directly applying CFG to one-step generation leads to blending artifacts and diminished output quality. To fill this gap, we introduce Negative-Away Steer Attention (NASA), an efficient method that integrates negative prompts into one-step diffusion models. NASA operates within the intermediate representation space by leveraging cross-attention mechanisms to suppress undesired visual attributes. This strategy avoids the blending artifacts inherent in output-space guidance and achieves high efficiency, incurring only a minimal 1.89\% increase in FLOPs compared to the computational doubling of CFG. Furthermore, NASA can be seamlessly integrated into existing timestep distillation frameworks, enhancing the student's output quality. Experimental results demonstrate that NASA substantially improves controllability and output quality, achieving an HPSv2 score of 31.21, setting a new state-of-the-art benchmark for one-step diffusion models.
  },
  pdf={https://arxiv.org/abs/2412.02687},
  website={https://snoopi-onestep.github.io/},
  selected={true},
  slides={/assets/publications/iccv2025/slides.pdf},
  poster={/assets/publications/iccv2025/poster.pdf},
  preview={iccv2025.jpg}
}

@article{le2025expressiveness,
  title={On the Expressiveness of Visual Prompt Experts},
  author={Minh Le* and Anh Nguyen* and Huy Nguyen and Chau Nguyen and Anh Tran and Nhat Ho},
  journal={arXiv preprint arXiv:2501.18936},
  year={2025},
  abbr={arXiv},
  pdf={https://arxiv.org/abs/2501.18936},
  abstract={Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.},
  selected={true}
}
