---
---

@inproceedings{nguyen2025improved,
  title={Improved Training Technique for Shortcut Models},
  author={Nguyen, Anh and Nguyen, Viet and Vu, Duc and Dao, Trung and Tran, Chi and Tran, Toan and Tran, Anh},
  booktitle={The Thirty-nine Annual Conference on Neural Information Processing Systems, 2025},
  year={2025},
  abbr={NeurIPS},
  abstract={
  This paper introduces improved training techniques for shortcut models to enhance their performance and efficiency.
  },
  selected={true}
}

@inproceedings{nguyen2025supercharged,
  title={Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts},
  author={Nguyen, Viet and Nguyen, Anh and Dao, Trung and Nguyen, Khoi and Pham, Cuong and Tran, Toan and Tran, Anh},
  booktitle={International Conference on Computer Vision},
  year={2025},
  abbr={ICCV},
  abstract={
  The escalating demand for real-time image synthesis has driven significant advancements in one-step diffusion models, which inherently offer expedited generation speeds compared to traditional multi-step methods. However, this enhanced efficiency is frequently accompanied by a compromise in the controllability of image attributes. While negative prompting, typically implemented via classifier-free guidance (CFG), has proven effective for fine-grained control in multi-step models, its application to one-step generators remains largely unaddressed. Due to the lack of iterative refinement, as in multi-step diffusion, directly applying CFG to one-step generation leads to blending artifacts and diminished output quality. To fill this gap, we introduce Negative-Away Steer Attention (NASA), an efficient method that integrates negative prompts into one-step diffusion models. NASA operates within the intermediate representation space by leveraging cross-attention mechanisms to suppress undesired visual attributes. This strategy avoids the blending artifacts inherent in output-space guidance and achieves high efficiency, incurring only a minimal 1.89\% increase in FLOPs compared to the computational doubling of CFG. Furthermore, NASA can be seamlessly integrated into existing timestep distillation frameworks, enhancing the student's output quality. Experimental results demonstrate that NASA substantially improves controllability and output quality, achieving an HPSv2 score of 31.21, setting a new state-of-the-art benchmark for one-step diffusion models.
  },
  pdf={https://arxiv.org/abs/2412.02687},
  html={https://snoopi-onestep.github.io/},
  selected={true}
}

@article{le2025expressiveness,
  title={On the Expressiveness of Visual Prompt Experts},
  author={\href{https://minhchuyentoancbn.github.io/}{Minh Le}* and Anh Nguyen* and \href{https://huynm99.github.io/}{Huy Nguyen} and Chau Nguyen and Anh Tran and \href{https://nhatptnk8912.github.io/}{Nhat Ho}},
  journal={arXiv preprint arXiv:2501.18936},
  year={2025},
  abbr={arXiv},
  pdf={https://arxiv.org/abs/2501.18936},
  abstract={Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.},
  selected={true}
}

@inproceedings{song2019generative,
  title={Generative Modeling by Estimating Gradients of the Data Distribution},
  author={Song, Yang and Ermon, Stefano},
  booktitle={the 33rd Conference on Neural Information Processing Systems, 2019.},
  year={2019},
  abbr={NeurIPS},
  award={Oral},
  honor={Oral Presentation [top 0.5\%]},
  abstract={
  We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients might be ill-defined when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.
  },
  pdf={https://arxiv.org/abs/1907.05600},
  code={https://github.com/ermongroup/ncsn},
  media={https://www.youtube.com/watch?v=Oc3X_x1Q1jU},
  poster={NeurIPS2019/ncsn-poster.pdf},
  slides={NeurIPS2019/talk_public.pptx},
  blog={https://yang-song.github.io/blog/2021/score/},
  selected={true}
}


@inproceedings{song2020improved,
  title={Improved Techniques for Training Score-Based Generative Models},
  author={Yang Song and Stefano Ermon},
  booktitle={the 34th Conference on Neural Information Processing Systems, 2020.},
  year={2020},
  abstract={
    Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.
  },
  pdf={https://arxiv.org/abs/2006.09011},
  code={https://github.com/ermongroup/ncsnv2},
  abbr={arXiv},
  blog={https://yang-song.github.io/blog/2021/score/},
  poster={NeurIPS2020/ncsnv2-poster.pdf},
  selected={true},
}
