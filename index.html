<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Anh Nguyen</title> <meta name="author" content="Anh Nguyen"/> <meta name="description" content="Anh Nguyen - Predoctoral Research Resident @ Qualcomm "/> <meta name="keywords" content="Anh Nguyen, machine learning, AI, artificial intelligence, deep learning, generative models, creative models, diffusion models"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ùìê</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://aengusng8.github.io//"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="col-sm-3"> <img src="/assets/img/profile.png" alt="Anh Nguyen profile photo" class="img-fluid z-depth-1 rounded-circle" style="width: 100%;"> </div> <div class="col-sm-9"> <h1 style="font-size: 2.8em; margin-bottom: 20px; color: #e6edf3; letter-spacing: -0.5px;"> <span style="font-weight: 800;">Anh</span> <span style="font-weight: 300; opacity: 0.9;">Nguyen</span> <span style="font-weight: 300; color: #58a6ff; font-size: 0.7em; vertical-align: middle;">/</span> <span style="font-weight: 800; color: #58a6ff; font-size: 0.7em; vertical-align: middle;">Aengus</span> </h1> <p class="desc"></p> <div style="font-size: 1.05em; line-height: 1.6; margin-bottom: 20px;"> I am a <span style="color: #58a6ff; font-weight: bold;">Predoctoral Research Resident</span> at <a href="https://www.qualcomm.com/research/artificial-intelligence" target="_blank" style="color: #58a6ff; text-decoration: none;" rel="noopener noreferrer">Qualcomm AI Research</a>, where I am advised by Principal Scientist <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&amp;hl=en" target="_blank" style="color: #58a6ff; text-decoration: none;" rel="noopener noreferrer">Dr. Anh Tran</a>. </div> <div style="background: rgba(255, 158, 100, 0.1); border-left: 3px solid #ff9e64; padding: 10px 15px; border-radius: 0 4px 4px 0; margin-bottom: 25px;"> <span style="color: #ff9e64; font-weight: bold;">üöÄ Fall 2026 Intake:</span> <span style="color: #e6edf3;">I am actively pursuing a <b>PhD position</b> in Computer Science and am excited to collaborate on impactful research!</span> </div> <span class="contact-info">Contact: anng@qti.qualcomm.com</span> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%6E%6E%67@%71%74%69.%71%75%61%6C%63%6F%6D%6D.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://twitter.com/aengusng8" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="https://www.quora.com/profile/Anh-Nguyen-Aengus" title="Quora" target="_blank" rel="noopener noreferrer"><i class="fab fa-quora"></i></a> </div> </div> </div> </div> <div class="clearfix"> <div style="padding: 2px; background: linear-gradient(45deg, #f960eb, #ff854d, #fff41f, #ff854d, #f960eb); border-radius: 12px; margin-bottom: 25px;"> <div style="background-color: #161b22; border-radius: 10px; padding: 20px; text-align: center;"> <span style="color: #e6edf3; font-style: italic; font-size: 1.1em;"> I work on <span style="color: #58a6ff; font-weight: bold;">deep generative modeling</span> as a principled route to <span style="color: #58a6ff; font-weight: bold;">machine intelligence beyond human levels</span>. </span> </div> </div> <div style="text-align: justify;"> <span style="color: #ff9e64; font-weight: bold; font-size: 1.1em;">Research Statement:</span> My <b>long-term goal</b> is to build systems capable of <i>understanding, reasoning, planning</i>, and <i>acquiring physical intuition</i> about the world. <br><br> My current research focuses on <b style="color: #58a6ff; font-weight: bold;">Efficient &amp; Robust Multimodal Intelligence</b>, aiming to resolve the trade-offs in foundation models through two core pillars: <b style="color: #58a6ff;">(1) Efficiency &amp; Scalability</b> to minimize training and inference costs, and <b style="color: #58a6ff;">(2) Robustness &amp; Controllability</b> to enforce alignment and reliability. <br><br> Most recently, my work on <b style="color: #58a6ff;">One-step Generative Modeling &amp; Distillation</b> (<b style="color: #58a6ff;">NeurIPS</b> &amp; <b style="color: #58a6ff;">ICCV</b> 2025) collapses iterative inference into <i>real-time, high-fidelity synthesis</i>, while my research on <b style="color: #58a6ff;">Multimodal Representation</b> (<b style="color: #58a6ff;">ICCV</b> 2025) leverages internal semantics for <i>zero-shot, fine-grained controllability</i>. <br><br> <b>Research Readiness:</b> I can <b>independently lead</b> the entire research lifecycle for top-tier conferences, driving projects from problem formulation and experimentation to final publication. </div> <hr style="border-color: #30363d; margin: 30px 0;"> <div style="text-align: justify;"> <span style="color: #ff9e64; font-weight: bold; font-size: 1.1em;">Previously:</span> I spent two years as a predoctoral research resident in the highly selective AI Residency Program at <a href="https://github.com/VinAIResearch" target="_blank" style="color: #d2a8ff; text-decoration: none;" rel="noopener noreferrer">VinAI Research</a>, a lab recognized in the world's top 20 for AI research based on its research output at top-tier conferences like CVPR and NeurIPS. <br><br> The program provided intensive, PhD-level training, during which I was responsible for the entire research lifecycle, from problem formulation and experimentation to final publication. This rigorous environment has a proven record of 119 PhD scholarships worldwide. The program's elite status was further underscored by Qualcomm's <a href="https://techcrunch.com/2025/04/01/qualcomm-acquires-generative-ai-division-of-vietnamese-startup-vinai/" target="_blank" style="color: #d2a8ff; text-decoration: none;" rel="noopener noreferrer">acquisition of its generative AI unit in 2025</a>. </div> <p><br></p> <div> <span style="color: #ff9e64; font-weight: bold; font-size: 1.1em;">Outside the Lab:</span> I enjoy the combination of mathematics, coding, and intuition. Away from the keyboard, you can find me clearing my mind on long-distance runs üèÉ‚Äç‚ôÇÔ∏è </div> <div class="news" style="margin-top: 40px;"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Oct 6, 2025</th> <td> üèÜ I am honored to receive the <span style="color: #ff854d; font-weight: normal;">Outstanding Resident in Research and Applied Demo Award 2025</span>! The award is part of the 2025 Recognition Awards from the Qualcomm AI Residency Program, which honors ‚Äúthe exceptional achievements of our residents this year.‚Äù </td> </tr> <tr> <th scope="row">Sep 18, 2025</th> <td> ‚ö° <a href="https://arxiv.org/abs/2510.21250v1" target="_blank" rel="noopener noreferrer">Improved Training Technique for Shortcut Models</a> got accepted at <strong class="aurora-text">NeurIPS 2025</strong>. This paper tackle the five core issues that held shortcut models back: the hidden <strong>flaw of compounding guidance</strong>, <strong>inflexible fixed guidance</strong>, <strong>frequency bias</strong>, <strong>divergent self-consistency</strong>, and <strong>curvy flow trajectories</strong>. Our method achieves <strong>state-of-the-art FID scores</strong>, making shortcut models a viable class of generative models capable of <strong>one-step, few-step, and multi-step sampling</strong>. </td> </tr> <tr> <th scope="row">Jun 26, 2025</th> <td> ‚ö° <a href="https://arxiv.org/abs/2412.02687" target="_blank" rel="noopener noreferrer">Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts</a> got accepted at <strong class="aurora-text">ICCV 2025</strong>. This paper, for the first time, enables <strong>negative guidance</strong> in <strong>one-step diffusion models</strong>, unlocking precise creative control without sacrificing speed. The proposed method boosts both <strong>controllability and quality</strong>, achieving a new <strong>state-of-the-art HPSv2 score</strong>. </td> </tr> </table> </div> </div> </div> <div class="publications"> <h2>selected publications <a href="publications/">[full list]</a> </h2> (*) denotes equal contribution <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/neurips2025.jpg"></div> <div class="col-sm-9"> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="nguyen2025improved" class="col-sm-10"> <div class="title">Improved Training Technique for Shortcut Models</div> <div class="author"> <em>Anh Nguyen</em>*,¬†Viet Nguyen*,¬†Duc Vu,¬†<a href="https://trung-dt.com/" target="_blank" rel="noopener noreferrer">Trung Dao</a>,¬†Chi Tran,¬†<a href="https://scholar.google.com.vn/citations?user=PnwSuNMAAAAJ&amp;hl=vi" target="_blank" rel="noopener noreferrer">Toan Tran</a>,¬†and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Anh Tran</a> </div> <div class="periodical"> <em>In The Thirty-nine Annual Conference on Neural Information Processing Systems, 2025</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2510.21250v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/publications/neurips2025/poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="/assets/publications/neurips2025/slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p> Shortcut models represent a promising, non-adversarial paradigm for generative modeling, uniquely supporting one-step, few-step, and multi-step sampling from a single trained network. However, their widespread adoption has been stymied by critical performance bottlenecks. This paper tackles the five core issues that held shortcut models back: (1) the hidden flaw of compounding guidance, which we are the first to formalize, causing severe image artifacts; (2) inflexible fixed guidance that restricts inference-time control; (3) a pervasive frequency bias driven by a reliance on low-level distances in the direct domain, which biases reconstructions toward low frequencies; (4) divergent self-consistency arising from a conflict with EMA training; and (5) curvy flow trajectories that impede convergence. To address these challenges, we introduce iSM, a unified training framework that systematically resolves each limitation. Our framework is built on four key improvements: Intrinsic Guidance provides explicit, dynamic control over guidance strength, resolving both compounding guidance and inflexibility. A Multi-Level Wavelet Loss mitigates frequency bias to restore high-frequency details. Scaling Optimal Transport (sOT) reduces training variance and learns straighter, more stable generative paths. Finally, a Twin EMA strategy reconciles training stability with self-consistency. Extensive experiments on ImageNet 256 √ó 256 demonstrate that our approach yields substantial FID improvements over baseline shortcut models across one-step, few-step, and multi-step generation, making shortcut models a viable and competitive class of generative models. </p> </div> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iccv2025.jpg"></div> <div class="col-sm-9"> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICCV</abbr></div> <div id="nguyen2025supercharged" class="col-sm-10"> <div class="title">Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts</div> <div class="author"> Viet Nguyen*,¬†<em>Anh Nguyen</em>*,¬†<a href="https://trung-dt.com/" target="_blank" rel="noopener noreferrer">Trung Dao</a>,¬†<a href="https://www.khoinguyen.org/" target="_blank" rel="noopener noreferrer">Khoi Nguyen</a>,¬†<a href="https://sites.google.com/view/cuongpham/home" target="_blank" rel="noopener noreferrer">Cuong Pham</a>,¬†<a href="https://scholar.google.com.vn/citations?user=PnwSuNMAAAAJ&amp;hl=vi" target="_blank" rel="noopener noreferrer">Toan Tran</a>,¬†and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Anh Tran</a> </div> <div class="periodical"> <em>In International Conference on Computer Vision, 2025</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2412.02687" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/publications/iccv2025/poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="/assets/publications/iccv2025/slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a> <a href="https://snoopi-onestep.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p> The escalating demand for real-time image synthesis has driven significant advancements in one-step diffusion models, which inherently offer expedited generation speeds compared to traditional multi-step methods. However, this enhanced efficiency is frequently accompanied by a compromise in the controllability of image attributes. While negative prompting, typically implemented via classifier-free guidance (CFG), has proven effective for fine-grained control in multi-step models, its application to one-step generators remains largely unaddressed. Due to the lack of iterative refinement, as in multi-step diffusion, directly applying CFG to one-step generation leads to blending artifacts and diminished output quality. To fill this gap, we introduce Negative-Away Steer Attention (NASA), an efficient method that integrates negative prompts into one-step diffusion models. NASA operates within the intermediate representation space by leveraging cross-attention mechanisms to suppress undesired visual attributes. This strategy avoids the blending artifacts inherent in output-space guidance and achieves high efficiency, incurring only a minimal 1.89% increase in FLOPs compared to the computational doubling of CFG. Furthermore, NASA can be seamlessly integrated into existing timestep distillation frameworks, enhancing the student‚Äôs output quality. Experimental results demonstrate that NASA substantially improves controllability and output quality, achieving an HPSv2 score of 31.21, setting a new state-of-the-art benchmark for one-step diffusion models. </p> </div> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"></div> <div class="col-sm-9"> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="le2025expressiveness" class="col-sm-10"> <div class="title">On the Expressiveness of Visual Prompt Experts</div> <div class="author"> Minh Le*,¬†<em>Anh Nguyen</em>*,¬†<a href="https://huynm99.github.io/" target="_blank" rel="noopener noreferrer">Huy Nguyen</a>,¬†Chau Nguyen,¬†<a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Anh Tran</a>,¬†and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2501.18936</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2501.18936" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.</p> </div> </div> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Anh Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <div style="display: none;"> <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=YnXmqVzu3eVosxvJvcNQ9_kPU0MPXN4aCxbHrGF8kTc&amp;cl=ffffff&amp;w=a"></script> <a href="https://clustrmaps.com/site/1c75b" title="ClustrMaps" target="_blank" rel="noopener noreferrer"><img src="//www.clustrmaps.com/map_v2.png?d=YnXmqVzu3eVosxvJvcNQ9_kPU0MPXN4aCxbHrGF8kTc&amp;cl=ffffff"></a> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-69768980-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-69768980-1");</script> </body> </html>